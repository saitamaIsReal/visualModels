{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90abfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a Jupytext‐compatible Python script using the “percent” format.\n",
    "# You can save this as `fsii_models.py` and then run:\n",
    "#   jupytext --to notebook fsii_models.py\n",
    "# to get a ready-to-run `.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed108e2",
   "metadata": {},
   "source": [
    "# FSII Analysis with Multiple Vision Models\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load several HuggingFace vision models (ViT, DeiT, custom).\n",
    "2. Split an input image into patches and visualize the grid.\n",
    "3. Define a masking function that grays out selected patches.\n",
    "4. Compute class logits on masked images.\n",
    "5. Approximate 2nd-order Faithful Shapley Interaction Indices (FSII) with SHAPIQ.\n",
    "6. Display the top‑5 interacting patch‑pairs on the image for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd86bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from PIL import Image, ImageDraw\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from typing import Union\n",
    "from shapiq.approximator.regression import RegressionFSII\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd4761",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# === Parameter & Device ===\n",
    "models = [\n",
    "    \"google/vit-base-patch32-384\",\n",
    "    \"facebook/deit-tiny-patch16-224\",\n",
    "    \"akahana/vit-base-cats-vs-dogs\",\n",
    "    # add more HF vision-model IDs here\n",
    "]\n",
    "\n",
    "image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\"\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\" CUDA device name:\", torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440fd062",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# === Helper functions ===\n",
    "\n",
    "def mask_image_grid(\n",
    "    img: Image.Image,\n",
    "    coalition: Union[np.ndarray, list],\n",
    "    image_size: int,\n",
    "    n_patches_per_row: int,\n",
    "    cell: int\n",
    ") -> Image.Image:\n",
    "    \"\"\"\n",
    "    Gray out patches where coalition[i] is False.\n",
    "    \"\"\"\n",
    "    arr  = np.array(img.resize((image_size, image_size))).copy()\n",
    "    coal = np.asarray(coalition, dtype=bool)\n",
    "    for i, keep in enumerate(coal):\n",
    "        if not keep:\n",
    "            r, c = divmod(i, n_patches_per_row)\n",
    "            y1, y2 = r*cell, (r+1)*cell\n",
    "            x1, x2 = c*cell, (c+1)*cell\n",
    "            arr[y1:y2, x1:x2] = 128\n",
    "    return Image.fromarray(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea6e31",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def draw_grid(\n",
    "    img: Image.Image,\n",
    "    n_patches_per_row: int,\n",
    "    cell: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Draw numeric patch-grid for visualization.\n",
    "    \"\"\"\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for r in range(n_patches_per_row):\n",
    "        for c in range(n_patches_per_row):\n",
    "            x1, y1 = c*cell, r*cell\n",
    "            x2, y2 = x1+cell, y1+cell\n",
    "            draw.rectangle([x1,y1,x2,y2], outline=\"gray\", width=1)\n",
    "            idx = r*n_patches_per_row + c\n",
    "            draw.text((x1+2, y1+2), str(idx), fill=\"gray\")\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b5ff7e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def value_function(\n",
    "    coalitions: np.ndarray,\n",
    "    processor,\n",
    "    model,\n",
    "    device,\n",
    "    image,\n",
    "    predicted_class,\n",
    "    image_size,\n",
    "    n_patches_per_row,\n",
    "    cell\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Given array (n_coalitions, n_patches), return logits for `predicted_class`.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for coalition in coalitions:\n",
    "        masked = mask_image_grid(image, coalition, image_size, n_patches_per_row, cell)\n",
    "        batch  = processor(images=masked, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            logit = model(**batch).logits[0, predicted_class].item()\n",
    "        out.append(logit)\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097ad951",
   "metadata": {},
   "source": [
    "## Main loop over models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e06892",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    print(f\"\\n--- Evaluating {model_name} ---\")\n",
    "\n",
    "    # 1) load processor & model\n",
    "    processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "    model     = AutoModelForImageClassification.from_pretrained(model_name).to(device).eval()\n",
    "\n",
    "    # 2) extract patch & image config\n",
    "    patch_size        = model.config.patch_size\n",
    "    image_size        = model.config.image_size\n",
    "    n_patches_per_row = image_size // patch_size\n",
    "    n_patches         = n_patches_per_row**2\n",
    "    cell = patch_size\n",
    "\n",
    "    # 3) load & show grid\n",
    "    resp  = requests.get(image_url)\n",
    "    image = Image.open(BytesIO(resp.content)).convert(\"RGB\").resize((image_size,image_size))\n",
    "    draw_grid(image.copy(), n_patches_per_row, cell)\n",
    "\n",
    "    # 4) determine target class\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_class = int(logits.argmax(-1))\n",
    "    print(\"Predicted class:\", model.config.id2label[predicted_class])\n",
    "\n",
    "    # 5) quick-check full vs. empty\n",
    "    full  = value_function(\n",
    "        np.array([np.ones(n_patches, bool)]),\n",
    "        processor, model, device, image,\n",
    "        predicted_class,\n",
    "        image_size, n_patches_per_row, cell\n",
    "    )[0]\n",
    "    empty = value_function(\n",
    "        np.array([np.zeros(n_patches, bool)]),\n",
    "        processor, model, device, image,\n",
    "        predicted_class,\n",
    "        image_size, n_patches_per_row, cell\n",
    "    )[0]\n",
    "    print(f\" Logit full:  {full:.2f}\")\n",
    "    print(f\" Logit empty: {empty:.2f}\")\n",
    "\n",
    "    # 6) FSII approx\n",
    "    approximator = RegressionFSII(\n",
    "        n=n_patches,\n",
    "        max_order=2,\n",
    "        pairing_trick=False,\n",
    "        random_state=42\n",
    "    )\n",
    "    result = approximator.approximate(\n",
    "        budget=300,\n",
    "        game=lambda c: value_function(\n",
    "            c,\n",
    "            processor, model, device, image,\n",
    "            predicted_class,\n",
    "            image_size, n_patches_per_row, cell\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 7) top‑5 2nd‑order interactions\n",
    "    fsii_map = result.dict_values\n",
    "    second   = {p:v for p,v in fsii_map.items() if len(p)==2}\n",
    "    top5     = sorted(second.items(), key=lambda kv: abs(kv[1]), reverse=True)[:5]\n",
    "    print(\" Top-5 FSII 2nd-order:\")\n",
    "    for i,(pair,val) in enumerate(top5,1):\n",
    "        print(f\"  {i}. {pair} → {val:.4f}\")\n",
    "\n",
    "    # 8) visualize on image\n",
    "    vis   = image.copy()\n",
    "    draw  = ImageDraw.Draw(vis)\n",
    "    colors= [\"red\",\"blue\",\"green\",\"yellow\",\"purple\"]\n",
    "    drawn = set()\n",
    "    for idx,(i,j) in enumerate([p for p,_ in top5]):\n",
    "        col = colors[idx]\n",
    "        for patch in (i,j):\n",
    "            if patch in drawn: continue\n",
    "            drawn.add(patch)\n",
    "            r, c = divmod(patch, n_patches_per_row)\n",
    "            x1,y1 = c*cell, r*cell\n",
    "            x2,y2 = x1+cell, y1+cell\n",
    "            draw.rectangle([x1,y1,x2,y2], outline=col, width=3)\n",
    "            draw.text((x1+2,y1+2), str(idx+1), fill=col)\n",
    "    display(vis)\n",
    "\n",
    "print(\"\\n✅ Done.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
